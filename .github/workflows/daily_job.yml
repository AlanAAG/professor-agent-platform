name: Daily Harvester & Refinery Pipeline

# When to run this workflow
on:
  # Run automatically every day at 2 AM UTC
  schedule:
    - cron: '30 14 * * *'
  
  # Also allow manual triggering from GitHub Actions tab
  workflow_dispatch:
    inputs:
      mode:
        description: 'Pipeline mode (daily or backlog)'
        required: false
        default: 'daily'
        type: choice
        options:
          - daily
          - backlog

# Define the jobs to run
jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      # Step 1: Download your repository code
      - name: Checkout repository
        uses: actions/checkout@v4
      
      # Step 2: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      # Step 3: Install Python dependencies
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          sudo apt-get update
          sudo apt-get install -y xvfb
      
      # Step 4: Install Google Chrome for Selenium (driver handled by Selenium Manager)
      - name: Install Google Chrome
        uses: browser-actions/setup-chrome@v1
      
      # Step 5: Create necessary directories
      - name: Create data directories
        run: |
          mkdir -p logs/error_screenshots
          mkdir -p data/raw_transcripts
          mkdir -p data/raw_static
          mkdir -p data/raw_pdfs
          mkdir -p /tmp/harvester_downloads

      # Step 6: Create a .env file for Pydantic Settings (kept local to runner)
      - name: Write .env from GitHub Secrets
        run: |
          cat > .env << 'EOF'
          GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}
          SUPABASE_URL=${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY=${{ secrets.SUPABASE_KEY }}
          COACH_USERNAME=${{ secrets.COACH_USERNAME }}
          COACH_PASSWORD=${{ secrets.COACH_PASSWORD }}
          HARVESTER_SELENIUM_HEADLESS=true
          HARVESTER_PAGE_LOAD_TIMEOUT=60
          HARVESTER_WAIT_TIMEOUT=30
          HARVESTER_SCREENSHOT_DIR=logs/error_screenshots
          HARVESTER_DOWNLOADS_DIR=/tmp/harvester_downloads
          EOF
      
      # Step 7: Run Pytest Unit/Integration Tests
      - name: Run Pytest Unit/Integration Tests
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
          xvfb-run -a pytest -q -x tests/

      # Step 8: Run Harvester Job
      - id: run_harvester_job
        name: Run Harvester Job
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          COACH_USERNAME: ${{ secrets.COACH_USERNAME }}
          COACH_PASSWORD: ${{ secrets.COACH_PASSWORD }}
          PIPELINE_MODE: ${{ github.event.inputs.mode || 'daily' }}
          HARVESTER_SELENIUM_HEADLESS: 'true'
        run: |
          bash ./run_daily_job.sh "$PIPELINE_MODE"
      
      # Step 9: Upload logs and screenshots (on failure)
      - name: Upload logs and screenshots (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-failure-${{ github.run_number }}
          path: |
            logs/
          if-no-files-found: ignore
          retention-days: 7

      # Step 10: Upload logs and screenshots (on success)
      - name: Upload logs and screenshots (on success)
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-success-${{ github.run_number }}
          path: |
            logs/
          if-no-files-found: ignore
          retention-days: 3

      # Step 11: Upload logs and screenshots (on cancellation)
      - name: Upload logs and screenshots (on cancellation)
        if: cancelled()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-cancelled-${{ github.run_number }}
          path: |
            logs/
          if-no-files-found: ignore
          retention-days: 7

      # Step 12: Ping Production API Health
      - name: Ping Production API Health
        if: always()
        env:
          PROD_URL: ${{ secrets.PRODUCTION_API_URL }}
        run: |
          if [ -z "${PROD_URL}" ]; then
            echo "⚠️  PRODUCTION_API_URL not set; skipping health ping." && exit 0
          fi
          echo "Pinging ${PROD_URL}/health ..."
          if curl -fsS --max-time 15 "${PROD_URL}/health" > /dev/null; then
            echo "✅ Health endpoint OK"
          else
            echo "Health endpoint not available; trying root / ..."
            curl -fsS --max-time 15 "${PROD_URL}/" > /dev/null
            echo "✅ Root endpoint OK"
          fi
      
      # Step 13: Clean up sensitive files
      - name: Clean up sensitive files
        if: always()
        run: |
          rm -f data/auth_state.json
          rm -f .env
          echo "✅ Cleaned up authentication files"